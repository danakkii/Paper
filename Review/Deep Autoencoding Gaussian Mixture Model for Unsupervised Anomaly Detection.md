# Abstract

<aside>
ğŸ’¡ 1. ë¶„ë¦¬ëœ ëª¨ë¸ í•™ìŠµì—ì„œ ì¼ê´€ì ì´ì§€ ì•Šì€ ìµœì í™” ëª©í‘œ
2. ì €ì°¨ì› ê³µê°„ì—ì„œ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì œëŒ€ë¡œ ë³´ì¡´ ëª»í•¨
compression network : ì €ì°¨ì› ê³µê°„ìœ¼ë¡œì„œì˜ ì°¨ì› ì¶•ì†Œ & reconstruction error ìƒì„±
â†’ deep autoencoder (non-linearity)
estimation network : compressed dataì˜ í™•ë¥ ë¶„í¬ ì¶”ì •
â†’ gaussian mixture model ì‚¬ìš©
autoencoderì™€ mixture modelì„ ë™ì‹œì— ê°™ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” end-to-end ë°©ì‹ ì œ
emëŒ€ì‹  joint optimisation í™œìš©

</aside>

# Unsupervised Anomaly Detection

[]()

one-class supervised anomaly detection ë°©ì‹ì€ ì •ìƒ sampleì´ í•„ìš”í•˜ë‹¤. ì–´ë–¤ ê²ƒì´ ì •ìƒ sampleì¸ì§€ ì•Œê¸° ìœ„í•´ì„œëŠ” ë°˜ë“œì‹œ ì •ìƒ sampleì— ëŒ€í•œ labelì„ í™•ë³´í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤. ëŒ€ë¶€ë¶„ì˜ ë°ì´í„°ê°€ ì •ìƒ sampleì´ë¼ëŠ” ê°€ì •ì„ í•˜ì—¬ label ì·¨ë“ì—†ì´ í•™ìŠµì„ ì‹œí‚¤ëŠ” unsupervised anomaly detection ë°©ë²•ë¡  ì—°êµ¬

ê°€ì¥ ë‹¨ìˆœí•˜ê²ŒëŠ” ì£¼ì–´ì§„ ë°ì´í„°ì— ëŒ€í•´ Principal Component Analysis(PCA, ì£¼ì„±ë¶„ ë¶„ì„)ë¥¼ ì´ìš©í•˜ì—¬ ì°¨ì›ì„ ì¶•ì†Œí•˜ê³  ë³µì›ì„ í•˜ëŠ” ê³¼ì •ì„ í†µí•´ ë¹„ì •ìƒ sampleì„ ê²€ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. , Neural Network ê¸°ë°˜ìœ¼ë¡œëŠ” ëŒ€í‘œì ìœ¼ë¡œ Autoencoder ê¸°ë°˜ì˜ ë°©ë²•ë¡ ì´ ì£¼ë¡œ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. AutoencoderëŠ” ì…ë ¥ì„ code í˜¹ì€ latent variableë¡œ ì••ì¶•í•˜ëŠ” Encodingê³¼, ì´ë¥¼ ë‹¤ì‹œ ì›ë³¸ê³¼ ê°€ê¹ê²Œ ë³µì›í•´ë‚´ëŠ” Decoding ê³¼ì •ìœ¼ë¡œ ì§„í–‰ì´ ë˜ë©° ì´ë¥¼ í†µí•´ ë°ì´í„°ì˜ ì¤‘ìš”í•œ ì •ë³´ë“¤ë§Œ ì••ì¶•ì ìœ¼ë¡œ ë°°ìš¸ ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ ë°ì´í„°ì˜ ì£¼ì„±ë¶„ì„ ë°°ìš¸ ìˆ˜ ìˆëŠ” PCAì™€ ìœ ì‚¬í•œ ë™ì‘ì„ í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Autoencoderë¥¼ ì´ìš©í•˜ë©´ ë°ì´í„°ì— ëŒ€í•œ labelingì„ í•˜ì§€ ì•Šì•„ë„ ë°ì´í„°ì˜ ì£¼ì„±ë¶„ì´ ë˜ëŠ” ì •ìƒ ì˜ì—­ì˜ íŠ¹ì§•ë“¤ì„ ë°°ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ, í•™ìŠµëœ autoencoderì— ì •ìƒ sampleì„ ë„£ì–´ì£¼ë©´ ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì˜ ë³µì›ì„ í•˜ë¯€ë¡œ inputê³¼ outputì˜ ì°¨ì´ê°€ ê±°ì˜ ë°œìƒí•˜ì§€ ì•ŠëŠ” ë°˜ë©´, ë¹„ì •ìƒì ì¸ sampleì„ ë„£ìœ¼ë©´ autoencoderëŠ” ì •ìƒ sampleì²˜ëŸ¼ ë³µì›í•˜ê¸° ë•Œë¬¸ì— inputê³¼ outputì˜ ì°¨ì´ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì—ì„œ ì°¨ì´ê°€ ë„ë“œë¼ì§€ê²Œ ë°œìƒí•˜ë¯€ë¡œ ë¹„ì •ìƒ sampleì„ ê²€ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¤ë§Œ Autoencoderì˜ code size (= latent variableì˜ dimension) ê°™ì€ hyper-parameterì— ë”°ë¼ ì „ë°˜ì ì¸ ë³µì› ì„±ëŠ¥ì´ ì¢Œìš°ë˜ê¸° ë•Œë¬¸ì— ì–‘/ë¶ˆ íŒì • ì •í™•ë„ê°€ Supervised Anomaly Detectionì— ë¹„í•´ ë‹¤ì†Œ ë¶ˆì•ˆì •í•˜ë‹¤ëŠ” ë‹¨ì ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ë˜í•œ autoencoderì— ë„£ì–´ì£¼ëŠ” inputê³¼ outputì˜ ì°¨ì´ë¥¼ ì–´ë–»ê²Œ ì •ì˜í•  ê²ƒì¸ì§€(= ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ difference mapì„ ê³„ì‚°í• ì§€) ì–´ëŠ loss functionì„ ì‚¬ìš©í•´ autoencoderë¥¼ í•™ìŠµì‹œí‚¬ì§€ ë“± ì—¬ëŸ¬ ê°€ì§€ ìš”ì¸ì— ë”°ë¼ ì„±ëŠ¥ì´ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ë“¯ ì„±ëŠ¥ì— ì˜í–¥ì„ ì£¼ëŠ” ìš”ì¸ì´ ë§ë‹¤ëŠ” ì•½ì ì´ ì¡´ì¬í•˜ì§€ë§Œ ë³„ë„ì˜ Labeling ê³¼ì • ì—†ì´ ì–´ëŠì •ë„ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ ì¥ë‹¨ì´ ëšœë ·í•œ ë°©ë²•ë¡ ì´ë¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ Autoencoderë¥¼ ì´ìš©í•˜ì—¬ Unsupervised Anomaly Detectionì„ ì ìš©í•˜ì—¬ Defect(ê²°í•¨)ì„ Segment í•˜ëŠ” ëŒ€í‘œì ì¸ ë…¼ë¬¸ë“¤ì—ì„œëŠ” Unsupervised ë°ì´í„° ì…‹ì´ ì¡´ì¬í•˜ì§€ ì•Šì•„ì„œ ì‹¤í—˜ì˜ í¸ì˜ë¥¼ ìœ„í•´ í•™ìŠµì— ì •ìƒ sampleë“¤ë§Œ ì‚¬ìš©í•˜ëŠ” Semi-Supervised Learning ë°©ì‹ì„ ì´ìš©í•˜ì˜€ìœ¼ë‚˜, Autoencoderë¥¼ ì´ìš©í•œ ë°©ë²•ë¡ ì€ Unsupervised Learning ë°©ì‹ì´ë©° Unsupervised ë°ì´í„° ì…‹ì—ë„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Autoencoder ê¸°ë°˜ Unsupervised Anomaly Detectionì„ ë‹¤ë£¬ ë…¼ë¬¸ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

- [**Improving Unsupervised Defect Segmentation by Applying Structural Similarity to Autoencoders**](https://arxiv.org/pdf/1807.02011.pdf)
- [**Deep Autoencoding Models for Unsupervised Anomaly Segmentation in Brain MR Images**](https://arxiv.org/pdf/1804.04488.pdf)
- [**MVTec AD â€“ A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection**](https://www.mvtec.com/fileadmin/Redaktion/mvtec.com/company/research/mvtec_ad.pdf)

Autoencoder ê¸°ë°˜ì˜ anomaly detection ë°©ë²•ë¡ ì— ëŒ€í•œ ì„¤ëª…ì€Â [**ë§ˆí‚¤ë‚˜ë½ìŠ¤ ê¹€ê¸°í˜„ë‹˜ ë¸”ë¡œê·¸ ê¸€**](https://kh-kim.github.io/blog/2019/12/15/Autoencoder-based-anomaly-detection.html)Â ì— ì˜ ì •ë¦¬ê°€ ë˜ì–´ìˆì–´ ë”°ë¡œ ë‹¤ë£¨ì§„ ì•Šì„ ì˜ˆì •ì…ë‹ˆë‹¤.

- ì¥ì : Labeling ê³¼ì •ì´ í•„ìš”í•˜ì§€ ì•Šë‹¤.
- ë‹¨ì : ì–‘/ë¶ˆ íŒì • ì •í™•ë„ê°€ ë†’ì§€ ì•Šê³  hyper parameterì— ë§¤ìš° ë¯¼ê°í•˜ë‹¤.

# joint optimization

1ê°œì˜ modelì—ì„œ nê°œì˜ outputì„ ì¶œë ¥í•œë‹¤ë©´ ì´ì— ë§ì¶° nê°œì˜ ì„œë¡œ ë‹¤ë¥¸ lossë¥¼ ë½‘ì„ ìˆ˜ ìˆë‹¤.

ì—¬ëŸ¬ ê°œì˜ lossë“¤ì„ í•˜ë‚˜ì˜ ê°’ìœ¼ë¡œ ë”í•´ì„œ ìµœì¢… lossë¡œ ì‚¬ìš©í•˜ëŠ” í›ˆë ¨ ë°©ì‹ì´ë‹¤.

ê° ì±„ë„ì´ ì„œë¡œ ë‹¤ë¥¸ ì—­í• ì„ ìˆ˜í–‰í•˜ë ¤ë©´ ì´ì— ë§ê²Œ loss functionì„ ì ìš©í•´ ì¤˜ì•¼ í•œë‹¤.

total loss = loss1 + loss2 + loss3 + loss4 + loss5

https://ballentain.tistory.com/30

The joint optimization, which well balances autoencoding reconstruction, density estimation of latent representation, and
regularization, helps the autoencoder escape from less attractive local optima and
further reduce reconstruction errors, avoiding the need of pre-training.

[]()

# alternate training

loss ê°œìˆ˜ì— ë§ê²Œ optimizerë¥¼ ìƒì„±í•´ ê° optimizerë¥¼ ë²ˆê°ˆì•„ ê°€ë©´ì„œ í•™ìŠµí•˜ëŠ” ë°©ì‹

[]()

# human supervision

ì‚¬ëŒì´ ë¶€ì—¬í•œ ì •ë‹µ

Although fruitful progress has been made in the last several years, conducting robust anomaly
detection on multi- or high-dimensional data without human supervision remains a challenging
task.

https://davinci-ai.tistory.com/10

# introduction

To address this issue caused by the
curse of dimensionality, two-step approaches are widely adopted (Candes et al. (2011)), in which `
dimensionality reduction is first conducted, and then density estimation is performed in the latent
low-dimensional space

1. two-step approaches
    1. dimensionality reduction
    2. density estimation

## suboptimal performance

ì°¨ì„  

ë­ì§€..

However, these approaches could easily lead to suboptimal performance,
because dimensionality reduction in the first step is unaware of the subsequent density estimation
task, and the key information for anomaly detection could be removed in the first place.

(2) anomalies are harder to
reconstruct, compared with normal samples. Unlike existing methods that only involve one of the
aspects (Zimek et al. (2012); Zhai et al. (2016)) with sub-optimal performance, DAGMM utilizes a
sub-network called compression network to perform dimensionality reduction by an autoencoder,
which prepares a low-dimensional representation for an input sample by concatenating reduced
low-dimensional features from encoding and the reconstruction error from decoding.

First, DAGMM preserves the key information of an input sample in a low-dimensional space that
includes features from both the reduced dimensions discovered by dimensionality reduction and
the induced reconstruction error. From the example shown in Figure 1, we can see that anomalies
differ from normal samples in two aspects: (1) anomalies can be significantly deviated in the reduced
dimensions where their features are correlated in a different way; and (2) anomalies are harder to
reconstruct, compared with normal samples. Unlike existing methods that only involve one of the
aspects (Zimek et al. (2012); Zhai et al. (2016)) with sub-optimal performance, DAGMM utilizes a
sub-network called compression network to perform dimensionality reduction by an autoencoder,
which prepares a low-dimensional representation for an input sample by concatenating reduced
low-dimensional features from encoding and the reconstruction error from decoding. 

Second, DAGMM leverages a Gaussian Mixture Model (GMM) over the learned low-dimensional
space to deal with density estimation tasks for input data with complex structures, which are yet
rather difficult for simple models used in existing works (Zhai et al. (2016)). While GMM has strong
capability, it also introduces new challenges in model learning. As GMM is usually learned by
alternating algorithms such as Expectation-Maximization (EM) (Huber (2011)), it is hard to perform
joint optimization of dimensionality reduction and density estimation favoring GMM learning, which
is often degenerated into a conventional two-step approach. To address this training challenge,
DAGMM utilizes a sub-network called estimation network that takes the low-dimensional input from
the compression network and outputs mixture membership prediction for each sample. With the
predicted sample membership, we can directly estimate the parameters of GMM, facilitating the
evaluation of the energy/likelihood of input samples. By simultaneously minimizing reconstruction
error from compression network and sample energy from estimation network, we can jointly train a
dimensionality reduction component that directly helps the targeted density estimation task.
Finally, DAGMM is friendly to end-to-end training. Usually, it is hard to learn deep autoencoders
by end-to-end training, as they can be easily stuck in less attractive local optima,

## Expectation-Maximization

- alternating algorithms

[https://modern-manual.tistory.com/entry/EM-ì•Œê³ ë¦¬ì¦˜-ì˜ˆì‹œë¡œ-ì‰½ê²Œ-ì´í•´í•˜ê¸°-Expectation-maximization-EM-algorithm](https://modern-manual.tistory.com/entry/EM-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%98%88%EC%8B%9C%EB%A1%9C-%EC%89%BD%EA%B2%8C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-Expectation-maximization-EM-algorithm)

ëª¨ìˆ˜ : ëª¨ì§‘ë‹¨ì˜ ìˆ˜ì¹˜ì  ìš”ì•½ê°’, ëª¨í‰ê· ì´ë‚˜ ëª¨í‘œì¤€í¸ì°¨ ê°™ì€ ëª¨ì§‘ë‹¨ì— ëŒ€í•œ í†µê³„ 

ì •ë‹µ labelì—†ì´ ì „í˜€ ì•Œì§€ ëª»í•˜ëŠ” hidden variableì— ëŒ€í•œ ê°’ì„ í’€ ìˆ˜ ìˆë‹¤ëŠ” íŠ¹

## memebrship

To address this training challenge,
DAGMM utilizes a sub-network called estimation network that takes the low-dimensional input from
the compression network and outputs mixture membership prediction for each sample. With the
predicted sample membership, we can directly estimate the parameters of GMM, facilitating the
evaluation of the energy/likelihood of input samples. 

## regularization

- W(weight)ê°€ ë„ˆë¬´ í° ê°’ë“¤ì„ ê°€ì§€ì§€ ì•Šë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤.
- Wê°€ ë„ˆë¬´ í° ê°’ì„ ê°€ì§€ê²Œ ë˜ë©´ ê³¼í•˜ê²Œ êµ¬ë¶ˆêµ¬ë¶ˆí•œ í˜•íƒœì˜ í•¨ìˆ˜ê°€ ë§Œë“¤ì–´ì§€ëŠ”ë°, Regularizationì€ ì´ëŸ° ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ë‚®ì¶”ê¸° ìœ„í•œ ë°©ë²•ì´ë‹¤.
- Regularizationì€ ë‹¨ìˆœí•˜ê²Œ cost functionì„ ì‘ì•„ì§€ëŠ” ìª½ìœ¼ë¡œ í•™ìŠµí•˜ë©´ íŠ¹ì • ê°€ì¤‘ì¹˜ ê°’ë“¤ì´ ì»¤ì§€ë©´ì„œ ê²°ê³¼ë¥¼ ë‚˜ì˜ê²Œ ë§Œë“¤ê¸° ë•Œë¬¸ì— cost functionì„ ë°”ê¾¼ë‹¤.

Our empirical study demonstrates
that, DAGMM is well-learned by the end-to-end training, as the regularization introduced by the
estimation network greatly helps the autoencoder in the compression network escape from less
attractive local optima.

## local optima

Our empirical study demonstrates
that, DAGMM is well-learned by the end-to-end training, as the regularization introduced by the
estimation network greatly helps the autoencoder in the compression network escape from less
attractive local optima.

https://simonezz.tistory.com/87

ëª¨ë¸ì€ íŠ¸ë ˆì´ë‹ ê³¼ì •ì—ì„œ loss functionì„ ìµœì í™”ì‹œí‚¤ë©´ì„œ í›ˆë ¨ëœë‹¤.

local optimaì— ë¹ ì§€ë©´ global optimaë¥¼ ì°¾ì•„ê°€ê¸° í˜ë“¤

## end-to-end train

ì…ë ¥ë¶€í„° ì¶œë ¥ê¹Œì§€ íŒŒì´í”„ë¼ì¸ ë„¤íŠ¸ì›Œí¬ ì—†ì´ ì‹ ê²½ë§ìœ¼ë¡œ í•œ ë²ˆì— ì²˜ë¦¬

- raw data â†’ neural network â†’ prediction

In addition, the end-to-end trained DAGMM significantly outperforms all the baseline methods that rely
on pre-trained autoencoders

https://velog.io/@jeewoo1025/What-is-end-to-end-deep-learning

[]()

ëª¨ë¸ì˜ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ê°€ í•˜ë‚˜ì˜ ì†ì‹¤í•¨ìˆ˜ì— ëŒ€í•´ ë™ì‹œì— í›ˆë ¨ë˜ëŠ” ê²½ë¡œê°€ ê°€ëŠ¥í•œ ë„¤íŠ¸ì›Œí¬ë¡œì¨ ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ (Backpropagation Algorithm) ê³¼ í•¨ê»˜ ìµœì í™” ë  ìˆ˜ ìˆë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.

ì˜ˆë¥¼ë“¤ì–´ ì¸ì½”ë”(ì–¸ì–´ì˜ ì…ë ¥ì„ ë²¡í„°ë¡œ)ì™€ ë””ì½”ë”(ë²¡í„°ì˜ ì…ë ¥ì„ ì–¸ì–´ë¡œ)ì—ì„œ ëª¨ë‘ê°€ ë™ì‹œì— í•™ìŠµë˜ëŠ” ê¸°ê³„ ë²ˆì—­ ë¬¸ì œì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ ì ìš© ë  ìˆ˜ ìˆë‹¤.

ì¦‰, ì‹ ê²½ë§ì€ í•œìª½ ëì—ì„œ ì…ë ¥ì„ ë°›ì•„ë“¤ì´ê³  ë‹¤ë¥¸ ìª½ ëì—ì„œ ì¶œë ¥ì„ ìƒì„±í•˜ëŠ”ë°, ì…ë ¥ ë° ì¶œë ¥ì„ ì§ì ‘ ê³ ë ¤í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ ê°€ì¤‘ì¹˜ë¥¼ ìµœì í™” í•˜ëŠ” í•™ìŠµì„ ì¢…ë‹¨ ê°„ í•™ìŠµ(End-to-end Learning) ì´ë¼ê³  í•œë‹¤.

## explicit linear projections

implicit non-linear projections 

Conventional methods in this category
include Principal Component Analysis (PCA) (Jolliffe (1986)) with explicit linear projections, kernel
PCA with implicit non-linear projections induced by specific kernels (Gunter et al.), and Robust PCA
(RPCA) (Huber (2011); Candes et al. (2011)) that makes PCA less sensitive to noise by enforcing `
sparse structures.

# reconstruction error

However, the performance of reconstruction based methods is limited by the fact that
they only conduct anomaly analysis from a single aspect, that is, reconstruction error.

Although the
compression on anomalous samples could be different from the compression on normal samples
and some of them do demonstrate unusually high reconstruction errors, a significant amount of
anomalous samples could also lurk with a normal level of error, which usually happens when the
underlying dimensionality reduction methods have high model complexity or the samples of interest
are noisy with complex structures. 

### anomaly detection with AE

1. ì…ë ¥ ìƒ˜í”Œì„ ì¸ì½”ë”ë¥¼ í†µí•´ ì €ì°¨ì›ìœ¼ë¡œ ì••ì¶•
2. ì••ì¶•ëœ ìƒ˜í”Œì„ ë””ì½”ë”ë¥¼ í†µê³¼ì‹œì¼œ ë‹¤ì‹œ ì›ë˜ ì°¨ì›ìœ¼ë¡œ ë³µì›
3. ì…ë ¥ ìƒ˜í”Œê³¼ ë³µì› ìƒ˜í”Œì˜ ë³µì› ì˜¤ì°¨(reconstruction error)ë¥¼ êµ¬í•¨
4. ë³µì› ì˜¤ì°¨ëŠ” ì´ìƒ ì ìˆ˜(anomaly score)ê°€ ë˜ì–´ thresholdì™€ ë¹„êµë¥¼ í†µí•´ ì´ìƒ ì—¬ë¶€ë¥¼ ê²°ì •
    1. threshold ë³´ë‹¤ í´ ê²½ìš° ì´ìƒìœ¼ë¡œ ê°„ì£¼
    2. threshold ë³´ë‹¤ ì‘ì€ ê²½ìš° ì •ìƒìœ¼ë¡œ ê°„ì£¼
    
    https://kh-kim.github.io/blog/2019/12/15/Autoencoder-based-anomaly-detection.html
    

Because of the
curse of dimensionality, it is difficult to directly apply such methods to multi- or high- dimensional
data. Traditional techniques adopt a two-step approach (Chandola et al. (2009)), where dimensionality reduction is conducted first, then clustering analysis is performed, and the two steps are separately learned.

1. dimensionality reduction
2. clustering analysis

To address this issue, recent
works propose deep autoencoder based methods in order to jointly learn dimensionality reduction
and clustering components â†’ deep autoencoderë¥¼ í†µí•´ ì°¨ì› ì¶•ì†Œì™€ êµ°ì§‘í™”ë¥¼ ì§„í–‰

However, the performance of the state-of-the-art methods is limited by
over-simplified clustering models that are unable to handle clustering or density estimation tasks for data of complex structures, or the pre-trained dimensionality reduction component (i.e., autoencoder) has little potential to accommodate further adjustment by the subsequent fine-tuning for anomaly detection. â†’ ê³¼í•˜ê²Œ ë‹¨ìˆœí™”ëœ êµ°ì§‘ ëª¨ë¸ì€ êµ°ì§‘í™”ê°€ í˜ë“¤ê³  ì°¨ì› ì¸¡ì •ì´ í˜ë“¤ê³  ì‚¬ì „í•™ìŠµëœ ì°¨ì› ì¶•ì†Œê°€ í˜ë“¤ë‹¤ 

DAGMM explicitly addresses these issues by a sub-network called estimation network
that evaluates sample density in the low-dimensional space produced by its compression network. â†’ DAGMMëŠ” í•´ê²°í•˜ê¸° ìœ„í•´ì„œ estimation networkë¡œ ë¶ˆë¦¬ëŠ” sub-networkë¡œ í•´ê²°í•œë‹¤. 

By predicting sample mixture membership, we are able to estimate the parameters of GMM without
EM-like alternating procedures. Moreover, DAGMM is friendly to end-to-end training so that we can unleash the full potential of adjusting dimensionality reduction components and jointly improve the quality of clustering analysis/density estimation.

â†’ sample mixture membershipì„ ì˜ˆì¸¡í•¨ìœ¼ë¡œì¨ alternating proceduresì—†ì´ gmm íŒŒë¼ë¯¸í„°ë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆë‹¤. 

DAGMM ì€ end-to-end training ì¹œí™”ì ì´ë¼ì„œ ì°¨ì› ì¶•ì†Œì™€ êµ°ì§‘í™”ë¥¼ ë” ì˜ í•  ìˆ˜ ìˆë‹¤.

# overview

Deep Autoencoding Gaussian Mixture Model (DAGMM) consists of two major components: a compression network and an estimation network.

1. compression network
2. estimation network

the compression network performs dimensionality reduction for input samples by a deep autoencoder, prepares their low-dimensional representations from both the reduced space and the reconstruction error features, and feeds the representations to the subsequent estimation network; (2) the estimation network takes the feed, and predicts their likelihood/energy in the framework of Gaussian Mixture Model (GMM).

## likelihood/energy

the estimation network takes the feed, and predicts their likelihood/energy in the framework of Gaussian Mixture Model (GMM).

With the predicted sample membership, we can directly estimate the parameters of GMM, facilitating the evaluation of the energy/likelihood of input samples.

https://jaejunyoo.blogspot.com/search/label/kr

https://velog.io/@pabiya/ENERGY-BASED-GENERATIVE-ADVERSARIAL-NETWORKS

https://velog.io/@jsshin2022/Paper-Review-Deep-Autoencoding-Gaussian-Mixture-Model-for-Unsupervised-Anomaly-Detection

https://iamseungjun.tistory.com/22

reconstruction errorë¥¼ energyë¡œ ê°„ì£¼í•œë‹¤. 

â†’ loss functionì´ë‘ ë¹„ìŠ·í•œ ê±´ê°€?

## estimation network

- sub-network

the estimation network estimates the parameters of GMM and evaluates the likelihood/energy for samples without alternating procedures such as EM (Zimek et al. (2012)). The estimation network achieves this by utilizing a multi-layer neural network to predict the mixture membership for each sample.
